{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.metrics import Accuracy, Recall, Precision\n",
    "from datetime import datetime, timedelta\n",
    "from pyedflib import EdfReader\n",
    "\n",
    "# private module\n",
    "from dilationmodel import dilationnet\n",
    "from read_dataset import GetBatchIndexes, LoadDataset, Interval2Segments_v2, Segments2Data_v2\n",
    "\n",
    "# to use gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(Sequence):\n",
    "    def __init__(self,type_1_data, type_2_data, type_3_data, batch_size):\n",
    "        \n",
    "        self.type_1_data = type_1_data\n",
    "        self.type_2_data = type_2_data\n",
    "        self.type_3_data = type_3_data\n",
    "\n",
    "        self.type_1_data_len = len(type_1_data)\n",
    "        self.type_2_data_len = len(type_2_data)\n",
    "        \n",
    "        type_3_sampled_for_balance = type_3_data[np.random.choice(len(type_3_data), int((self.type_1_data_len + self.type_2_data_len)*1.5),replace=False)]\n",
    "        self.type_3_data_len = len(type_3_sampled_for_balance)\n",
    "\n",
    "        self.batch_num = int((self.type_1_data_len + self.type_2_data_len + self.type_3_data_len)/batch_size)\n",
    "\n",
    "        self.type_1_batch_indexes = GetBatchIndexes(self.type_1_data_len, self.batch_num)\n",
    "        self.type_2_batch_indexes = GetBatchIndexes(self.type_2_data_len, self.batch_num)\n",
    "        self.type_3_batch_indexes = GetBatchIndexes(self.type_3_data_len, self.batch_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seg = np.concatenate((self.type_1_data[self.type_1_batch_indexes[idx]], self.type_2_data[self.type_2_batch_indexes[idx]], self.type_3_data[self.type_3_batch_indexes[idx]]))\n",
    "\n",
    "        batch_x, batch_y = Segments2Data_v2(input_seg)\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''        \n",
    "    def __init__(self, type_1, type_2, type_3, batch_size, shuffle=True):\n",
    "        self.type_1 = type_1\n",
    "        self.type_2 = type_2\n",
    "        self.type_3 = type_3\n",
    "        self.batch_size = batch_size\n",
    "        print(type(self.type_1))\n",
    "        self.type_3_ratio = 1.5\n",
    "        self.type_3_used_size = int((len(self.type_1)+len(self.type_2))*self.type_3_ratio)\n",
    "    \n",
    "        if self.type_3_used_size<len(self.type_3):\n",
    "            type_3_index = np.random.choice(len(self.type_3), self.type_3_used_size, replace=False)\n",
    "            self.type_3_used = [self.type_3[i] for i in type_3_index]\n",
    "        else:\n",
    "            self.type_3_used = self.type_3\n",
    "    \n",
    "        self.data = np.concatenate((self.type_1, self.type_2, self.type_3_used))\n",
    "        np.random.shuffle(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.ceil(len(self.data)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        low = idx * self.batch_size\n",
    "        high = min(low + self.batch_size, len(self.x))        \n",
    "        input_seg = self.data[low:high]\n",
    "        batch_x, batch_y = Segments2Data_v2(input_seg)\n",
    "\n",
    "        print(f'data gen: {idx}')\n",
    "        print(batch_x.shape, batch_y.shape)\n",
    "        return batch_x, batch_y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "overlap_sliding_size = 1\n",
    "normal_sliding_size = window_size\n",
    "state = ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early', 'postictal','interictal']\n",
    "\n",
    "# for WSL\n",
    "train_info_file_path = \"./patient_info_chb_train.csv\"\n",
    "test_info_file_path = \"./patient_info_chb_test.csv\"\n",
    "edf_file_path = \"./data/CHB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interval_set = LoadDataset(train_info_file_path)\n",
    "train_segments_set = {}\n",
    "\n",
    "test_interval_set = LoadDataset(test_info_file_path)\n",
    "test_segments_set = {}\n",
    "\n",
    "# 상대적으로 데이터 갯수가 적은 것들은 window_size 2초에 sliding_size 1초로 overlap 시켜 데이터 증강\n",
    "for state in ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early']:\n",
    "    train_segments_set[state] = Interval2Segments_v2(train_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "    test_segments_set[state] = Interval2Segments_v2(test_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "    \n",
    "\n",
    "for state in ['postictal', 'interictal']:\n",
    "    train_segments_set[state] = Interval2Segments_v2(train_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "    test_segments_set[state] = Interval2Segments_v2(test_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "\n",
    "# type 1은 True Label데이터 preictal_ontime\n",
    "# type 2는 특별히 갯수 맞춰줘야 하는 데이터\n",
    "# type 3는 나머지\n",
    "\n",
    "# AutoEncoder 단계에서는 1:1:3\n",
    "\n",
    "train_type_1 = np.array(train_segments_set['preictal_ontime']) # true\n",
    "train_type_2 = np.array(train_segments_set['ictal'] + train_segments_set['preictal_early'] + train_segments_set['preictal_late']) # false\n",
    "train_type_3 = np.array(train_segments_set['postictal'] + train_segments_set['interictal']) # false\n",
    "\n",
    "test_type_1 = np.array(test_segments_set['preictal_ontime']) # true\n",
    "test_type_2 = np.array(test_segments_set['ictal'] + test_segments_set['preictal_early'] + test_segments_set['preictal_late']) # false\n",
    "test_type_3 = np.array(test_segments_set['postictal'] + test_segments_set['interictal']) # false\n",
    "\n",
    "fold_n = 5\n",
    "kf = KFold(n_splits=fold_n, shuffle=True)\n",
    "epochs = 10\n",
    "batch_size = 1000   # 한번의 gradient update시마다 들어가는 데이터의 사이즈\n",
    "\n",
    "type_1_kfold_set = kf.split(train_type_1)\n",
    "type_2_kfold_set = kf.split(train_type_2)\n",
    "type_3_kfold_set = kf.split(train_type_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(fold_n):\n",
    "    (type_1_train_indexes, type_1_val_indexes) = next(type_1_kfold_set)\n",
    "    (type_2_train_indexes, type_2_val_indexes) = next(type_2_kfold_set)\n",
    "    (type_3_train_indexes, type_3_val_indexes) = next(type_3_kfold_set)\n",
    "    \n",
    "    checkpoint_path = f\"Cov1d_training_{_}/cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    if os.path.exists(f\"./Cov1d_training_{_+1}\"):\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.exists(f\"./Cov1d_training_{_}\"):\n",
    "            print(\"Model Loaded!\")\n",
    "            model = tf.keras.models.load_model(checkpoint_path)\n",
    "        else:\n",
    "            inputs = Input(shape=(18,512))\n",
    "            outputs = dilationnet(inputs)\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "        model.compile(optimizer = 'Adam',\n",
    "                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "                      metrics=['acc'])\n",
    "        \n",
    "    logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    '''\n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                    histogram_freq = 1,\n",
    "                                                    profile_batch = (1, 400))\n",
    "    '''\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_best_only=True,\n",
    "                                                    verbose=1)\n",
    "    \n",
    "    train_generator = Dataloader(train_type_1[type_1_train_indexes], train_type_2[type_2_train_indexes], train_type_3[type_3_train_indexes], batch_size)\n",
    "    validation_generator = Dataloader(train_type_1[type_1_val_indexes], train_type_2[type_2_val_indexes], train_type_3[type_3_val_indexes], batch_size)\n",
    "    history = model.fit(\n",
    "                train_generator,\n",
    "                epochs = epochs,\n",
    "                validation_data = validation_generator,\n",
    "                #use_multiprocessing=True,\n",
    "                #workers=6,\n",
    "                callbacks= [#tboard_callback,\n",
    "                            cp_callback]\n",
    "                )\n",
    "    with open(f'./Cov1d_training_{_}/trainHistoryDict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
