{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class autoencoder_generator(Sequence):\n",
    "    def __init__(self,type_1_data, type_2_data, type_3_data, batch_size):\n",
    "        \n",
    "        self.type_1_data = type_1_data\n",
    "        self.type_2_data = type_2_data\n",
    "        self.type_3_data = type_3_data\n",
    "\n",
    "        self.type_1_data_len = len(type_1_data)\n",
    "        self.type_2_data_len = len(type_2_data)\n",
    "        \n",
    "        type_3_sampled_for_balance = type_3_data[np.random.choice(len(type_3_data), int((self.type_1_data_len + self.type_2_data_len)*1.5),replace=False)]\n",
    "        self.type_3_data_len = len(type_3_sampled_for_balance)\n",
    "\n",
    "        self.batch_num = int((self.type_1_data_len + self.type_2_data_len + self.type_3_data_len)/batch_size)\n",
    "\n",
    "        self.type_1_batch_indexes = GetBatchIndexes(self.type_1_data_len, self.batch_num)\n",
    "        self.type_2_batch_indexes = GetBatchIndexes(self.type_2_data_len, self.batch_num)\n",
    "        self.type_3_batch_indexes = GetBatchIndexes(self.type_3_data_len, self.batch_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seg = np.concatenate((self.type_1_data[self.type_1_batch_indexes[idx]], self.type_2_data[self.type_2_batch_indexes[idx]], self.type_3_data[self.type_3_batch_indexes[idx]]))\n",
    "        X_batch = Segments2Data(input_seg)\n",
    "        #X_batch = np.random.standard_normal((300,21,512))\n",
    "        return X_batch, X_batch\n",
    "\n",
    "# %%\n",
    "if __name__=='__main__':\n",
    "    window_size = 2\n",
    "    overlap_sliding_size = 1\n",
    "    normal_sliding_size = window_size\n",
    "    state = ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early', 'postictal','interictal']\n",
    "\n",
    "    # for WSL\n",
    "    train_info_file_path = \"/host/d/SNU_DATA/patient_info_train.csv\"\n",
    "    test_info_file_path = \"/host/d/SNU_DATA/patient_info_test.csv\"\n",
    "    edf_file_path = \"/host/d/SNU_DATA\"\n",
    "\n",
    "    ## for window\n",
    "    # train_info_file_path = \"D:/SNU_DATA/patient_info_train.csv\"\n",
    "    # test_info_file_path = \"D:/SNU_DATA/patient_info_test.csv\"\n",
    "    # edf_file_path = \"D:/SNU_DATA\"\n",
    "\n",
    "\n",
    "    train_interval_set = LoadDataset(train_info_file_path)\n",
    "    train_segments_set = {}\n",
    "\n",
    "    test_interval_set = LoadDataset(test_info_file_path)\n",
    "    test_segments_set = {}\n",
    "\n",
    "    # 상대적으로 데이터 갯수가 적은 것들은 window_size 2초에 sliding_size 1초로 overlap 시켜 데이터 증강\n",
    "    for state in ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        \n",
    "\n",
    "    for state in ['postictal', 'interictal']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "\n",
    "    # type 1은 True Label데이터 preictal_ontime\n",
    "    # type 2는 특별히 갯수 맞춰줘야 하는 데이터\n",
    "    # type 3는 나머지\n",
    "\n",
    "    # AutoEncoder 단계에서는 1:1:3\n",
    "\n",
    "    train_type_1 = np.array(train_segments_set['preictal_ontime'])\n",
    "    train_type_2 = np.array(train_segments_set['ictal'] + train_segments_set['preictal_early'] + train_segments_set['preictal_late'])\n",
    "    train_type_3 = np.array(train_segments_set['postictal'] + train_segments_set['interictal'])\n",
    "\n",
    "    test_type_1 = np.array(test_segments_set['preictal_ontime'])\n",
    "    test_type_2 = np.array(test_segments_set['ictal'] + test_segments_set['preictal_early'] + test_segments_set['preictal_late'])\n",
    "    test_type_3 = np.array(test_segments_set['postictal'] + test_segments_set['interictal'])\n",
    "\n",
    "    fold_n = 5\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    epochs = 100\n",
    "    batch_size = 500   # 한번의 gradient update시마다 들어가는 데이터의 사이즈\n",
    "    total_len = len(train_type_1)+len(train_type_2)\n",
    "    total_len = int(total_len*2.5) # 데이터 비율 2:2:6\n",
    "\n",
    "    type_1_kfold_set = kf.split(train_type_1)\n",
    "    type_2_kfold_set = kf.split(train_type_2)\n",
    "    type_3_kfold_set = kf.split(train_type_3)\n",
    "\n",
    "\n",
    "    for _ in range(fold_n):\n",
    "        (type_1_train_indexes, type_1_val_indexes) = next(type_1_kfold_set)\n",
    "        (type_2_train_indexes, type_2_val_indexes) = next(type_2_kfold_set)\n",
    "        (type_3_train_indexes, type_3_val_indexes) = next(type_3_kfold_set)\n",
    "        if os.path.exists(f\"./AutoEncoder_training_{_+1}\"):\n",
    "            if os.path.exists(f\"./AutoEncoder_training_{_}\"):\n",
    "                autoencoder_model = tf.keras.models.create_model()\n",
    "                autoencoder_model = tf.keras.models.load_model(f\"/AutoEncoder_training_{_}/cp.ckpt\")\n",
    "            else:\n",
    "                encoder_inputs = Input(shape=(21,512,1))\n",
    "                encoder_outputs = FullChannelEncoder(encoded_feature_num=64,inputs = encoder_inputs)\n",
    "                decoder_outputs = FullChannelDecoder(encoder_outputs)\n",
    "                autoencoder_model = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
    "                autoencoder_model.compile(optimizer = 'Adam', loss='mse',)\n",
    "            \n",
    "\n",
    "        type_1_data_len = len(type_1_train_indexes)\n",
    "        type_2_data_len = len(type_2_train_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        train_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)\n",
    "\n",
    "        type_1_data_len = len(type_1_val_indexes)\n",
    "        type_2_data_len = len(type_2_val_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        val_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)\n",
    "        logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        \n",
    "\n",
    "        tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                        histogram_freq = 1,\n",
    "                                                        profile_batch = '1,20')\n",
    "        checkpoint_path = f\"AutoEncoder_training_{_}/cp.ckpt\"\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                        save_best_only=True,\n",
    "                                                        verbose=1)\n",
    "        \n",
    "        train_generator = autoencoder_generator(train_type_1[type_1_train_indexes], train_type_2[type_2_train_indexes], train_type_3[type_3_train_indexes],batch_size)\n",
    "        validation_generator = autoencoder_generator(train_type_1[type_1_val_indexes], train_type_2[type_2_val_indexes], train_type_3[type_3_val_indexes],batch_size)\n",
    "# %%\n",
    "        history = autoencoder_model.fit_generator(\n",
    "                    train_generator,\n",
    "                    epochs = epochs,\n",
    "                    steps_per_epoch =  train_batch_num,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = val_batch_num,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    callbacks= [ tboard_callback, cp_callback ]\n",
    "                    )\n",
    "        \n",
    "        with open(f'./AutoEncoder_training_{_}/trainHistoryDict', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyedflib.highlevel import read_edf, read_edf_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2 # sec\n",
    "OVERLAP = 0 # sec\n",
    "FREQUENCY_CHB = 256 #Hz\n",
    "ictal_section_name = ['ictal', 'preictal_late', 'preictal_ontime', 'preictal_early', 'postictal', 'interictal']\n",
    "\n",
    "origin = pd.read_csv(\"./patient_info_chb_split.csv\")\n",
    "\n",
    "patient_info_chb_segment = list()\n",
    "\n",
    "for state in ictal_section_name:\n",
    "    total_current_state = origin[origin['state']==state]\n",
    "\n",
    "    # short-term data with overlap\n",
    "    if state in ['ictal', 'preictal_late', 'preictal_ontime', 'preictal_early']:\n",
    "        OVERLAP = 1\n",
    "\n",
    "    # long-term data without overlap\n",
    "    if state in [\"interictal\", \"postictal\"]:\n",
    "        OVERLAP = 0\n",
    "\n",
    "    for current_state in total_current_state.itertuples():\n",
    "        filename, start, end = current_state[1], current_state[2], current_state[3]\n",
    "\n",
    "        if end-start<WINDOW_SIZE:\n",
    "            continue\n",
    "        \n",
    "        dirname = filename.split('_')[0]\n",
    "        filepath = os.path.join('./data/CHB/',dirname,filename+'.edf')\n",
    "        \n",
    "        header = read_edf_header(filepath)\n",
    "        startdate = header['startdate']\n",
    "        \n",
    "        start_from_0sec = int(start-startdate.timestamp())*FREQUENCY_CHB\n",
    "        end_from_0sec = int(end-startdate.timestamp())*FREQUENCY_CHB            \n",
    "\n",
    "        step_size = (WINDOW_SIZE-OVERLAP)*FREQUENCY_CHB #index\n",
    "        window_size_with_frequency = int(WINDOW_SIZE*FREQUENCY_CHB)\n",
    "        for window_start_index in range(start_from_0sec, end_from_0sec, step_size):\n",
    "            # [\"name\", \"start\", \"duration\", \"state\", \"frequency\"]\n",
    "            current_segment = [filename, window_start_index, window_size_with_frequency, state, FREQUENCY_CHB]\n",
    "            patient_info_chb_segment.append(current_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770539\n"
     ]
    }
   ],
   "source": [
    "print(len(patient_info_chb_segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(patient_info_chb_segment, columns=[\"name\", \"start\", \"duration\", \"state\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257581, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['state']=='ictal'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:df.shape[0]//2,:]\n",
    "df2 = df.iloc[df.shape[0]//2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./patient_info_chb_segment_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('./patient_info_chb_segment_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyedflib import EdfReader\n",
    "from pyedflib.highlevel import read_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1953602 ,  0.1953602 ,  0.1953602 ,  0.1953602 ,  0.97680098,\n",
       "        0.58608059, -1.75824176, -1.36752137,  2.93040293,  4.1025641 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_segment(path, chn, start, n):\n",
    "    \"\"\"\n",
    "    Returns the physical data of signal chn. When start and n is set, a subset is returned\n",
    "\n",
    "    Parameters:\t\n",
    "    chn : int\n",
    "    channel number\n",
    "\n",
    "    start : int\n",
    "    start pointer (default is 0)\n",
    "\n",
    "    n : int\n",
    "    length of data to read (default is None, by which the complete data of the channel are returned)\n",
    "\n",
    "    digital: bool\n",
    "    will return the signal in original digital values instead of physical values\n",
    "\n",
    "    Examples\n",
    "    ——–\n",
    "    >>> import pyedflib\n",
    "    >>> f = pyedflib.data.test_generator()\n",
    "    >>> x = f.readSignal(0,0,1000)\n",
    "    >>> int(x.shape[0])\n",
    "    1000\n",
    "    >>> x2 = f.readSignal(0)\n",
    "    >>> int(x2.shape[0])\n",
    "    120000\n",
    "    >>> f.close()    \n",
    "    \"\"\"\n",
    "    path = \"./data/CHB/CHB001/CHB001_01.edf\"\n",
    "    f = EdfReader(path)\n",
    "    segment = f.readSignal(chn,start,n,digital=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.45934066e+02,  1.95360195e-01,  1.95360195e-01, ...,\n",
       "        -1.15262515e+01, -2.93040293e+00,  1.93406593e+01],\n",
       "       [-1.04517705e+02,  1.95360195e-01,  1.95360195e-01, ...,\n",
       "         2.36385836e+01,  2.75457875e+01,  3.06715507e+01],\n",
       "       [-4.27838828e+01,  1.95360195e-01,  1.95360195e-01, ...,\n",
       "         4.86446886e+01,  4.51282051e+01,  3.45787546e+01],\n",
       "       ...,\n",
       "       [-2.64713065e+02,  1.95360195e-01,  5.86080586e-01, ...,\n",
       "         9.76800977e-01, -1.58241758e+01, -2.94993895e+01],\n",
       "       [ 9.47496947e+01,  1.95360195e-01,  1.95360195e-01, ...,\n",
       "        -7.22832723e+00, -1.03540904e+01, -1.34798535e+01],\n",
       "       [ 4.47374847e+01,  1.95360195e-01,  1.95360195e-01, ...,\n",
       "         1.69963370e+01,  2.24664225e+01,  2.63736264e+01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
